<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ablation.ablation API documentation</title>
<meta name="description" content="ablation.py
About: Load ablation methods for Ablation" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ablation.ablation</code></h1>
</header>
<section id="section-intro">
<p>ablation.py
About: Load ablation methods for Ablation</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
ablation.py
About: Load ablation methods for Ablation
&#34;&#34;&#34;
from copy import deepcopy
from itertools import chain
from typing import Dict, List

import numpy as np
import pandas as pd

from .dataset import NumpyDataset
from .utils.evaluate import append_dict_lists, eval_model_performance
from .utils.transform import le_to_ohe, ohe_to_le


def local_explanation_importance(
    exp: np.ndarray, relative=False
) -&gt; np.ndarray:
    &#34;&#34;&#34;Average absolute value of ranked local explanations. For example, the first
        value in the returned array is the average of largest local absolute feature
        importance across all samples.

    Args:
        exp (np.ndarray): local explanations of shape (samples, features)
        relative (bool): return importance relative to sum of all features

    Returns:
        np.ndarray: average importance of ranked local explanations
    &#34;&#34;&#34;
    abs_exp = np.abs(exp)
    if relative:
        abs_exp = abs_exp / abs_exp.sum(-1, keepdims=True)
    ordered_exp = np.sort(abs_exp, -1)[:, ::-1]
    avg_importance = ordered_exp.mean(0)
    return avg_importance


class Ablation:
    &#34;&#34;&#34;Class to perform Ablations using feature importance&#34;&#34;&#34;

    def __init__(
        self,
        perturbation_distribution: np.ndarray,
        model,
        dataset: NumpyDataset,
        X: np.ndarray,
        y: np.ndarray,
        explanation_values: np.ndarray,
        explanation_values_dense: np.ndarray,
        random_feat_idx: np.ndarray = None,
        local: bool = True,
        scoring_methods: List[str] = [&#34;log_loss&#34;, &#34;abs_diff&#34;, &#34;auroc&#34;],
        scoring_step: float = 0,
    ):
        &#34;&#34;&#34;Constructor for ablation

        Args:
            perturbation_distribution (np.ndarray): perturbed dataset as a numpy arr
            model ([type]): a pre-trained model
            X (np.ndarray): original features dataset
            y (np.ndarray): target values
            explanation_values (np.ndarray): explanation values used for feature importance (eg. shap values)
            random_feat_idx (np.ndarray): random feature index
            local (bool): If True, perturbs data based on rank ordered local rather than global explanations.
                          Defaults to True.
            scoring_methods (List[str]): List of scoring methods (&#39;log_loss&#39;,&#39;abs_diff&#39;, &#39;auroc&#39;, &#39;aupr&#39;)
            scoring_step (float): Fraction of features to ablate in-between model evaluations (1, 0]. If 0, will
                                  evaluate at every feature.

        &#34;&#34;&#34;

        self.perturbation_distribution = perturbation_distribution
        self.model = model
        self.dataset = deepcopy(dataset)
        self.X = deepcopy(X)
        self.y = deepcopy(y)
        self.explanation_values = explanation_values
        self.explanation_values_dense = explanation_values_dense
        self.random_feat_idx = random_feat_idx
        self.local = local

        for s in scoring_methods:
            assert s in [
                &#34;auroc&#34;,
                &#34;aupr&#34;,
                &#34;log_loss&#34;,
                &#34;abs_diff&#34;,
            ], f&#34;{s} is not a valid scoring method! &#34;
        self.scoring_methods = scoring_methods

        assert (
            scoring_step &lt; 1 and scoring_step &gt;= 0
        ), &#34;scoring step must be between 0 and 1&#34;

        self.scoring_step = scoring_step

    def _steps(self):
        &#34;&#34;&#34;Calculate steps at which to evaluate model performance
            1. perturb dataset at atleast every feature
            2. record model evaluation at each score step
            3. provide normalized pct step for each evaluation point

        Returns:
            Tuple[np.ndarray,np.ndarray,np.ndarray]: perturb steps, score steps, percent steps
        &#34;&#34;&#34;

        # n_features = self.X.shape[1]
        n_features = len(self.dataset.original_feature_names)
        # fraction of features included at each step
        if self.scoring_step == 0:
            # If scoring_step is 0 default to scoring at each feature
            pct_steps = np.arange(0, n_features + 1) / n_features
        else:
            pct_steps = np.arange(0, 1 + self.scoring_step, self.scoring_step)
        # number of features at which to provide evaluation (exclude initial evaluation with all features)
        score_steps = (pct_steps * (n_features - 1)).astype(int)[1:]

        # Number of perturbations at minimum = n_features
        # Additional repeat steps taken if number of scoring steps&gt; n_features
        perturb_steps = (
            score_steps
            if len(score_steps) &gt; n_features
            else np.arange(n_features)
        )

        return perturb_steps, score_steps, pct_steps

    def ablate_features(self, plot=False) -&gt; Dict[str, np.ndarray]:
        &#34;&#34;&#34;Main function for ablation

        Args:
            plot (bool, optional): Plot ablation. Defaults to False.

        Returns:
            np.ndarray: scores, normalized scores, and percent of feature steps
        &#34;&#34;&#34;
        perturb_steps, score_steps, pct_steps = self._steps()
        ranked_features = self._sorted_feature_rankings()

        # Convert dataset to LE from OHE
        self.X = ohe_to_le(self.X, self.dataset.agg_map)

        scores = {s: [] for s in self.scoring_methods}
        score_no_perturbation = self._get_model_performance()
        scores = append_dict_lists(scores, score_no_perturbation)

        indx = np.arange(len(self.X))
        # Replace features in order of importance with perturbation, calculate and store new score
        for i in perturb_steps:
            # Replace at feature_indx (i-th highest ) with perturbation distribution
            feature_idx = ranked_features[i]
            self.X[indx, feature_idx] = self.perturbation_distribution[
                indx, feature_idx
            ]
            if i in score_steps:
                score_perturbed = self._get_model_performance()
                scores = append_dict_lists(scores, score_perturbed)

        n_steps = len(pct_steps)
        n_scores = len(self.scoring_methods)

        scores = {k: np.array(s) for k, s in scores.items()}
        score_change = {
            k: np.concatenate([s[1:] - s[:-1], [0]]) for k, s in scores.items()
        }
        exp_importance = np.concatenate(
            [local_explanation_importance(self.explanation_values_dense), [0]]
        )

        results = {
            &#34;pct_steps&#34;: list(pct_steps) * n_scores,
            &#34;score_name&#34;: sum([[k] * n_steps for k in scores], []),
            &#34;scores&#34;: np.concatenate(list(scores.values())),
            &#34;score_change&#34;: np.concatenate(list(score_change.values())),
            &#34;exp_importance&#34;: list(exp_importance) * n_scores,
        }

        # Restore Data Format
        # NOTE Test if self.X here equals self.dataset.X
        # self.X = le_to_ohe(self.X,self.dataset.agg_map)

        return pd.DataFrame(results)

    def _get_model_performance(self) -&gt; np.ndarray:
        &#34;&#34;&#34;Return performance for a model&#39;s paredicted probabilities

        Returns:
            float: model performance
        &#34;&#34;&#34;
        return {
            s: eval_model_performance(
                self.model, le_to_ohe(self.X, self.dataset.agg_map), self.y, s
            )
            for s in self.scoring_methods
        }

    def _sorted_feature_rankings(self):
        &#34;&#34;&#34;Generate ranked feature list, given explanation values

        Returns:
            np.array: Array of feature indices, in order of feature importance
                     If local, will return with shape (features, samples)
        &#34;&#34;&#34;

        # vals = np.abs(self.explanation_values)
        vals = np.abs(self.explanation_values_dense)
        if not self.local:
            vals = vals.mean(0)

        return np.argsort(-vals).transpose()

    def random_sanity_check_idx(self) -&gt; float:
        &#34;&#34;&#34;Sanity check for random features

        Global: first index where a random feature appears in ranked global feature importance
        Local: minimum weighted rank among all random features

        Returns:
            float: random feature rank, random feature percent rank
        &#34;&#34;&#34;

        if self.random_feat_idx is None:
            return self.X.shape[1], 1.0

        ranked_features = self._sorted_feature_rankings()
        if self.local:

            min_rank = len(ranked_features)
            for rand_feat_idx in self.random_feat_idx:
                is_random_feat = ranked_features == rand_feat_idx
                weighted_random_rank = sum(
                    is_random_feat.sum(1)
                    * np.arange(len(is_random_feat))
                    / is_random_feat.sum()
                )
                min_rank = min(min_rank, weighted_random_rank)

            # median_rank = np.where(
            #     is_random_feat.sum(1).cumsum() &gt; is_random_feat.sum() / 2
            # )[0].min()

            return min_rank, min_rank / self.X.shape[1]

        is_random_feat = np.in1d(ranked_features, self.random_feat_idx)
        random_rank = np.where(is_random_feat)[0].min()

        return random_rank, random_rank / self.X.shape[1]

    def random_sanity_check_value(self) -&gt; float:
        &#34;&#34;&#34;Sanity check for random feature explanation values

        Returns:
            float: Maximum value of most important random feature global explanation
        &#34;&#34;&#34;

        if self.random_feat_idx is None:
            return 0

        global_importance = np.abs(self.explanation_values).mean(0)
        random_feat_importance = global_importance[self.random_feat_idx].max()

        return random_feat_importance</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ablation.ablation.local_explanation_importance"><code class="name flex">
<span>def <span class="ident">local_explanation_importance</span></span>(<span>exp: numpy.ndarray, relative=False) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Average absolute value of ranked local explanations. For example, the first
value in the returned array is the average of largest local absolute feature
importance across all samples.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>exp</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>local explanations of shape (samples, features)</dd>
<dt><strong><code>relative</code></strong> :&ensp;<code>bool</code></dt>
<dd>return importance relative to sum of all features</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>average importance of ranked local explanations</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def local_explanation_importance(
    exp: np.ndarray, relative=False
) -&gt; np.ndarray:
    &#34;&#34;&#34;Average absolute value of ranked local explanations. For example, the first
        value in the returned array is the average of largest local absolute feature
        importance across all samples.

    Args:
        exp (np.ndarray): local explanations of shape (samples, features)
        relative (bool): return importance relative to sum of all features

    Returns:
        np.ndarray: average importance of ranked local explanations
    &#34;&#34;&#34;
    abs_exp = np.abs(exp)
    if relative:
        abs_exp = abs_exp / abs_exp.sum(-1, keepdims=True)
    ordered_exp = np.sort(abs_exp, -1)[:, ::-1]
    avg_importance = ordered_exp.mean(0)
    return avg_importance</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ablation.ablation.Ablation"><code class="flex name class">
<span>class <span class="ident">Ablation</span></span>
<span>(</span><span>perturbation_distribution: numpy.ndarray, model, dataset: <a title="ablation.dataset.NumpyDataset" href="dataset.html#ablation.dataset.NumpyDataset">NumpyDataset</a>, X: numpy.ndarray, y: numpy.ndarray, explanation_values: numpy.ndarray, explanation_values_dense: numpy.ndarray, random_feat_idx: numpy.ndarray = None, local: bool = True, scoring_methods: List[str] = ['log_loss', 'abs_diff', 'auroc'], scoring_step: float = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Class to perform Ablations using feature importance</p>
<p>Constructor for ablation</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>perturbation_distribution</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>perturbed dataset as a numpy arr</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>[type]</code></dt>
<dd>a pre-trained model</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>original features dataset</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>target values</dd>
<dt><strong><code>explanation_values</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>explanation values used for feature importance (eg. shap values)</dd>
<dt><strong><code>random_feat_idx</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>random feature index</dd>
<dt><strong><code>local</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, perturbs data based on rank ordered local rather than global explanations.
Defaults to True.</dd>
<dt><strong><code>scoring_methods</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>List of scoring methods ('log_loss','abs_diff', 'auroc', 'aupr')</dd>
<dt><strong><code>scoring_step</code></strong> :&ensp;<code>float</code></dt>
<dd>Fraction of features to ablate in-between model evaluations (1, 0]. If 0, will
evaluate at every feature.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Ablation:
    &#34;&#34;&#34;Class to perform Ablations using feature importance&#34;&#34;&#34;

    def __init__(
        self,
        perturbation_distribution: np.ndarray,
        model,
        dataset: NumpyDataset,
        X: np.ndarray,
        y: np.ndarray,
        explanation_values: np.ndarray,
        explanation_values_dense: np.ndarray,
        random_feat_idx: np.ndarray = None,
        local: bool = True,
        scoring_methods: List[str] = [&#34;log_loss&#34;, &#34;abs_diff&#34;, &#34;auroc&#34;],
        scoring_step: float = 0,
    ):
        &#34;&#34;&#34;Constructor for ablation

        Args:
            perturbation_distribution (np.ndarray): perturbed dataset as a numpy arr
            model ([type]): a pre-trained model
            X (np.ndarray): original features dataset
            y (np.ndarray): target values
            explanation_values (np.ndarray): explanation values used for feature importance (eg. shap values)
            random_feat_idx (np.ndarray): random feature index
            local (bool): If True, perturbs data based on rank ordered local rather than global explanations.
                          Defaults to True.
            scoring_methods (List[str]): List of scoring methods (&#39;log_loss&#39;,&#39;abs_diff&#39;, &#39;auroc&#39;, &#39;aupr&#39;)
            scoring_step (float): Fraction of features to ablate in-between model evaluations (1, 0]. If 0, will
                                  evaluate at every feature.

        &#34;&#34;&#34;

        self.perturbation_distribution = perturbation_distribution
        self.model = model
        self.dataset = deepcopy(dataset)
        self.X = deepcopy(X)
        self.y = deepcopy(y)
        self.explanation_values = explanation_values
        self.explanation_values_dense = explanation_values_dense
        self.random_feat_idx = random_feat_idx
        self.local = local

        for s in scoring_methods:
            assert s in [
                &#34;auroc&#34;,
                &#34;aupr&#34;,
                &#34;log_loss&#34;,
                &#34;abs_diff&#34;,
            ], f&#34;{s} is not a valid scoring method! &#34;
        self.scoring_methods = scoring_methods

        assert (
            scoring_step &lt; 1 and scoring_step &gt;= 0
        ), &#34;scoring step must be between 0 and 1&#34;

        self.scoring_step = scoring_step

    def _steps(self):
        &#34;&#34;&#34;Calculate steps at which to evaluate model performance
            1. perturb dataset at atleast every feature
            2. record model evaluation at each score step
            3. provide normalized pct step for each evaluation point

        Returns:
            Tuple[np.ndarray,np.ndarray,np.ndarray]: perturb steps, score steps, percent steps
        &#34;&#34;&#34;

        # n_features = self.X.shape[1]
        n_features = len(self.dataset.original_feature_names)
        # fraction of features included at each step
        if self.scoring_step == 0:
            # If scoring_step is 0 default to scoring at each feature
            pct_steps = np.arange(0, n_features + 1) / n_features
        else:
            pct_steps = np.arange(0, 1 + self.scoring_step, self.scoring_step)
        # number of features at which to provide evaluation (exclude initial evaluation with all features)
        score_steps = (pct_steps * (n_features - 1)).astype(int)[1:]

        # Number of perturbations at minimum = n_features
        # Additional repeat steps taken if number of scoring steps&gt; n_features
        perturb_steps = (
            score_steps
            if len(score_steps) &gt; n_features
            else np.arange(n_features)
        )

        return perturb_steps, score_steps, pct_steps

    def ablate_features(self, plot=False) -&gt; Dict[str, np.ndarray]:
        &#34;&#34;&#34;Main function for ablation

        Args:
            plot (bool, optional): Plot ablation. Defaults to False.

        Returns:
            np.ndarray: scores, normalized scores, and percent of feature steps
        &#34;&#34;&#34;
        perturb_steps, score_steps, pct_steps = self._steps()
        ranked_features = self._sorted_feature_rankings()

        # Convert dataset to LE from OHE
        self.X = ohe_to_le(self.X, self.dataset.agg_map)

        scores = {s: [] for s in self.scoring_methods}
        score_no_perturbation = self._get_model_performance()
        scores = append_dict_lists(scores, score_no_perturbation)

        indx = np.arange(len(self.X))
        # Replace features in order of importance with perturbation, calculate and store new score
        for i in perturb_steps:
            # Replace at feature_indx (i-th highest ) with perturbation distribution
            feature_idx = ranked_features[i]
            self.X[indx, feature_idx] = self.perturbation_distribution[
                indx, feature_idx
            ]
            if i in score_steps:
                score_perturbed = self._get_model_performance()
                scores = append_dict_lists(scores, score_perturbed)

        n_steps = len(pct_steps)
        n_scores = len(self.scoring_methods)

        scores = {k: np.array(s) for k, s in scores.items()}
        score_change = {
            k: np.concatenate([s[1:] - s[:-1], [0]]) for k, s in scores.items()
        }
        exp_importance = np.concatenate(
            [local_explanation_importance(self.explanation_values_dense), [0]]
        )

        results = {
            &#34;pct_steps&#34;: list(pct_steps) * n_scores,
            &#34;score_name&#34;: sum([[k] * n_steps for k in scores], []),
            &#34;scores&#34;: np.concatenate(list(scores.values())),
            &#34;score_change&#34;: np.concatenate(list(score_change.values())),
            &#34;exp_importance&#34;: list(exp_importance) * n_scores,
        }

        # Restore Data Format
        # NOTE Test if self.X here equals self.dataset.X
        # self.X = le_to_ohe(self.X,self.dataset.agg_map)

        return pd.DataFrame(results)

    def _get_model_performance(self) -&gt; np.ndarray:
        &#34;&#34;&#34;Return performance for a model&#39;s paredicted probabilities

        Returns:
            float: model performance
        &#34;&#34;&#34;
        return {
            s: eval_model_performance(
                self.model, le_to_ohe(self.X, self.dataset.agg_map), self.y, s
            )
            for s in self.scoring_methods
        }

    def _sorted_feature_rankings(self):
        &#34;&#34;&#34;Generate ranked feature list, given explanation values

        Returns:
            np.array: Array of feature indices, in order of feature importance
                     If local, will return with shape (features, samples)
        &#34;&#34;&#34;

        # vals = np.abs(self.explanation_values)
        vals = np.abs(self.explanation_values_dense)
        if not self.local:
            vals = vals.mean(0)

        return np.argsort(-vals).transpose()

    def random_sanity_check_idx(self) -&gt; float:
        &#34;&#34;&#34;Sanity check for random features

        Global: first index where a random feature appears in ranked global feature importance
        Local: minimum weighted rank among all random features

        Returns:
            float: random feature rank, random feature percent rank
        &#34;&#34;&#34;

        if self.random_feat_idx is None:
            return self.X.shape[1], 1.0

        ranked_features = self._sorted_feature_rankings()
        if self.local:

            min_rank = len(ranked_features)
            for rand_feat_idx in self.random_feat_idx:
                is_random_feat = ranked_features == rand_feat_idx
                weighted_random_rank = sum(
                    is_random_feat.sum(1)
                    * np.arange(len(is_random_feat))
                    / is_random_feat.sum()
                )
                min_rank = min(min_rank, weighted_random_rank)

            # median_rank = np.where(
            #     is_random_feat.sum(1).cumsum() &gt; is_random_feat.sum() / 2
            # )[0].min()

            return min_rank, min_rank / self.X.shape[1]

        is_random_feat = np.in1d(ranked_features, self.random_feat_idx)
        random_rank = np.where(is_random_feat)[0].min()

        return random_rank, random_rank / self.X.shape[1]

    def random_sanity_check_value(self) -&gt; float:
        &#34;&#34;&#34;Sanity check for random feature explanation values

        Returns:
            float: Maximum value of most important random feature global explanation
        &#34;&#34;&#34;

        if self.random_feat_idx is None:
            return 0

        global_importance = np.abs(self.explanation_values).mean(0)
        random_feat_importance = global_importance[self.random_feat_idx].max()

        return random_feat_importance</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="ablation.ablation.Ablation.ablate_features"><code class="name flex">
<span>def <span class="ident">ablate_features</span></span>(<span>self, plot=False) ‑> Dict[str, numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Main function for ablation</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>plot</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Plot ablation. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>scores, normalized scores, and percent of feature steps</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ablate_features(self, plot=False) -&gt; Dict[str, np.ndarray]:
    &#34;&#34;&#34;Main function for ablation

    Args:
        plot (bool, optional): Plot ablation. Defaults to False.

    Returns:
        np.ndarray: scores, normalized scores, and percent of feature steps
    &#34;&#34;&#34;
    perturb_steps, score_steps, pct_steps = self._steps()
    ranked_features = self._sorted_feature_rankings()

    # Convert dataset to LE from OHE
    self.X = ohe_to_le(self.X, self.dataset.agg_map)

    scores = {s: [] for s in self.scoring_methods}
    score_no_perturbation = self._get_model_performance()
    scores = append_dict_lists(scores, score_no_perturbation)

    indx = np.arange(len(self.X))
    # Replace features in order of importance with perturbation, calculate and store new score
    for i in perturb_steps:
        # Replace at feature_indx (i-th highest ) with perturbation distribution
        feature_idx = ranked_features[i]
        self.X[indx, feature_idx] = self.perturbation_distribution[
            indx, feature_idx
        ]
        if i in score_steps:
            score_perturbed = self._get_model_performance()
            scores = append_dict_lists(scores, score_perturbed)

    n_steps = len(pct_steps)
    n_scores = len(self.scoring_methods)

    scores = {k: np.array(s) for k, s in scores.items()}
    score_change = {
        k: np.concatenate([s[1:] - s[:-1], [0]]) for k, s in scores.items()
    }
    exp_importance = np.concatenate(
        [local_explanation_importance(self.explanation_values_dense), [0]]
    )

    results = {
        &#34;pct_steps&#34;: list(pct_steps) * n_scores,
        &#34;score_name&#34;: sum([[k] * n_steps for k in scores], []),
        &#34;scores&#34;: np.concatenate(list(scores.values())),
        &#34;score_change&#34;: np.concatenate(list(score_change.values())),
        &#34;exp_importance&#34;: list(exp_importance) * n_scores,
    }

    # Restore Data Format
    # NOTE Test if self.X here equals self.dataset.X
    # self.X = le_to_ohe(self.X,self.dataset.agg_map)

    return pd.DataFrame(results)</code></pre>
</details>
</dd>
<dt id="ablation.ablation.Ablation.random_sanity_check_idx"><code class="name flex">
<span>def <span class="ident">random_sanity_check_idx</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Sanity check for random features</p>
<p>Global: first index where a random feature appears in ranked global feature importance
Local: minimum weighted rank among all random features</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>random feature rank, random feature percent rank</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_sanity_check_idx(self) -&gt; float:
    &#34;&#34;&#34;Sanity check for random features

    Global: first index where a random feature appears in ranked global feature importance
    Local: minimum weighted rank among all random features

    Returns:
        float: random feature rank, random feature percent rank
    &#34;&#34;&#34;

    if self.random_feat_idx is None:
        return self.X.shape[1], 1.0

    ranked_features = self._sorted_feature_rankings()
    if self.local:

        min_rank = len(ranked_features)
        for rand_feat_idx in self.random_feat_idx:
            is_random_feat = ranked_features == rand_feat_idx
            weighted_random_rank = sum(
                is_random_feat.sum(1)
                * np.arange(len(is_random_feat))
                / is_random_feat.sum()
            )
            min_rank = min(min_rank, weighted_random_rank)

        # median_rank = np.where(
        #     is_random_feat.sum(1).cumsum() &gt; is_random_feat.sum() / 2
        # )[0].min()

        return min_rank, min_rank / self.X.shape[1]

    is_random_feat = np.in1d(ranked_features, self.random_feat_idx)
    random_rank = np.where(is_random_feat)[0].min()

    return random_rank, random_rank / self.X.shape[1]</code></pre>
</details>
</dd>
<dt id="ablation.ablation.Ablation.random_sanity_check_value"><code class="name flex">
<span>def <span class="ident">random_sanity_check_value</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Sanity check for random feature explanation values</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>Maximum value of most important random feature global explanation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_sanity_check_value(self) -&gt; float:
    &#34;&#34;&#34;Sanity check for random feature explanation values

    Returns:
        float: Maximum value of most important random feature global explanation
    &#34;&#34;&#34;

    if self.random_feat_idx is None:
        return 0

    global_importance = np.abs(self.explanation_values).mean(0)
    random_feat_importance = global_importance[self.random_feat_idx].max()

    return random_feat_importance</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ablation" href="index.html">ablation</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ablation.ablation.local_explanation_importance" href="#ablation.ablation.local_explanation_importance">local_explanation_importance</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ablation.ablation.Ablation" href="#ablation.ablation.Ablation">Ablation</a></code></h4>
<ul class="">
<li><code><a title="ablation.ablation.Ablation.ablate_features" href="#ablation.ablation.Ablation.ablate_features">ablate_features</a></code></li>
<li><code><a title="ablation.ablation.Ablation.random_sanity_check_idx" href="#ablation.ablation.Ablation.random_sanity_check_idx">random_sanity_check_idx</a></code></li>
<li><code><a title="ablation.ablation.Ablation.random_sanity_check_value" href="#ablation.ablation.Ablation.random_sanity_check_value">random_sanity_check_value</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>