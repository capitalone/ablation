<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ablation.distributions API documentation</title>
<meta name="description" content="perturb.py
About: Feature distributions for baselines and perturbations" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ablation.distributions</code></h1>
</header>
<section id="section-intro">
<p>perturb.py
About: Feature distributions for baselines and perturbations</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
perturb.py
About: Feature distributions for baselines and perturbations
&#34;&#34;&#34;
from typing import Optional

import numpy as np
from numpy.random import permutation, randn
from scipy import stats
from scipy.ndimage import gaussian_filter
from sklearn import base
from sklearn.neighbors import NearestNeighbors

from .utils.general import sample
from .utils.transform import le_to_ohe, ohe_to_le

# Main differences between perturbations and baselines:
# Baselines -- will be smaller samples
# Perturbations -- size of test set

ONE_TO_ONE = [&#34;max_distance&#34;]
MANY_TO_ONE = [
    &#34;nearest_neighbors&#34;,
    &#34;nearest_neighbors_counterfactual&#34;,
    &#34;opposite_class&#34;,
]
SAMPLE = [
    &#34;gaussian&#34;,
    &#34;gaussian_blur&#34;,
    &#34;gaussian_blur_permutation&#34;,
    &#34;training&#34;,
    &#34;marginal&#34;,
]
CONSTANT = [&#34;constant&#34;, &#34;constant_mean&#34;, &#34;constant_median&#34;]


def categorical_perturbation_case(**kwargs) -&gt; bool:
    &#34;&#34;&#34;
    Check for categoricals that need special care
    during distribution generation.  The categoricals
    should only be treated this way during perturbation.
    Baselines should use the default format.

    Returns:
        bool: Boolean indicating if distribution should be
              handling categorical features another way.
    &#34;&#34;&#34;
    if (
        (&#34;agg_map&#34; in kwargs)
        and (&#34;baseline&#34; not in kwargs)
        and (kwargs[&#34;agg_map&#34;] != None)
    ):
        return True
    return False


def constant(
    X: np.ndarray, value: Optional[float] = 0.0, **kwargs
) -&gt; np.ndarray:
    &#34;&#34;&#34;Generate a constant distribution

    Args:
        X (np.ndarray): Data to get shape
        value (float, optional): Constant value. Defaults to 0.0.

    Returns:
        np.array: constant distribution
    &#34;&#34;&#34;
    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        return np.ones((1, X_LE.shape[1])) * value

    return np.ones((1, X.shape[1])) * value


def constant_mean(X: np.ndarray, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34;Generate a constant mean distribution (mean value per feature)

    Args:
        X (np.ndarray): data to derive mean

    Returns:
        np.array: constant mean distribution
    &#34;&#34;&#34;
    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        return np.mean(X_LE, axis=0, keepdims=True)

    return np.mean(X, axis=0, keepdims=True)


def constant_median(X: np.ndarray, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34;Generate a constant median distribution (median value per feature)

    Args:
        X (np.ndarray): data to derive median

    Returns:
        np.array: constant median distribution
    &#34;&#34;&#34;
    if categorical_perturbation_case(**kwargs):
        # For this case, median is calculated over
        # numerical features and mode is calculated
        # over the categorical features.
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        median_mode = np.zeros((1, X_LE.shape[1]))  # To keep dimensions
        for (idx, mapping) in enumerate(kwargs[&#34;agg_map&#34;]):
            if len(mapping) &gt; 1:
                median_mode[0][idx] = stats.mode(X_LE[:, idx], axis=0)[0][
                    0
                ].astype(int)
            else:
                median_mode[0][idx] = np.median(X_LE[:, idx], axis=0)
        return median_mode

    return np.median(X, axis=0, keepdims=True)


def max_distance(X: np.ndarray, X_obs: np.ndarray, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34;Furthest valid data sample in L1 distance

    Args:
        X (np.ndarray): data to derive min/max
        X_obs (np.ndarray): data observations for which to generate max_distance

    Returns:
        np.array: furthest valid data samples by L1 distance
    &#34;&#34;&#34;

    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        X_obs_LE = ohe_to_le(X_obs, kwargs[&#34;agg_map&#34;])

        max_value = np.tile(X_LE.max(axis=0), (len(X_obs_LE), 1))
        min_value = np.tile(X_LE.min(axis=0), (len(X_obs_LE), 1))
        midpoint = (min_value + max_value) / 2

        # Maximum distance implemented for numericals.
        # Categoricals are uniformly sampled instead.
        modified_max_distance = np.zeros(
            (X_obs_LE.shape[0], X_obs_LE.shape[1])
        )

        for (idx, mapping) in enumerate(kwargs[&#34;agg_map&#34;]):
            if len(mapping) &gt; 1:
                # Replace categorical with uniformly random draw of the
                # other potential categories.

                unique_vals = np.unique(X_LE[:, idx])
                ps = np.apply_along_axis(
                    lambda x, y: np.setdiff1d(y, x),
                    1,
                    X_obs_LE[:, idx, np.newaxis],
                    unique_vals,
                )
                replacements = np.apply_along_axis(np.random.choice, 1, ps)
                modified_max_distance[:, idx] = replacements
            else:
                modified_max_distance[:, idx] = np.where(
                    X_obs_LE[:, idx] &lt; midpoint[:, idx],
                    max_value[:, idx],
                    min_value[:, idx],
                )
        return modified_max_distance

    max_value = np.tile(X.max(axis=0), (len(X_obs), 1))
    min_value = np.tile(X.min(axis=0), (len(X_obs), 1))
    midpoint = (min_value + max_value) / 2
    return np.where(X_obs &lt; midpoint, max_value, min_value)


def gaussian(X: np.ndarray, sigma: int, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34;Gaussian noise distribution

    Args:
        X (np.ndarray): source data
        sigma (int): sd of noise

    Returns:
        np.array: noisy source data
    &#34;&#34;&#34;

    if categorical_perturbation_case(**kwargs):
        raise NotImplementedError(
            &#34;Gaussian perturbation has not been implemented for categorical feature types.&#34;
        )

    return np.clip(randn(*X.shape) * sigma + X, a_min=X.min(), a_max=X.max())


def gaussian_blur(X: np.ndarray, sigma: int, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34;Generate gaussian blur over features

    Args:
        X (np.ndarray): source data for guassian blur
        sigma (int): Gaussian filter sigma

    Returns:
        np.ndarray: blurred data
    &#34;&#34;&#34;

    if categorical_perturbation_case(**kwargs):
        raise NotImplementedError(
            &#34;Gaussian Blur perturbation has not been implemented for categorical feature types.&#34;
        )

    return gaussian_filter(X, sigma=sigma)


def marginal(X: np.ndarray, X_obs: np.ndarray, **kwargs) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Sample over marginal distribution

    Args:
        X (np.ndarray): Source data for marginals
        X_obs (np.ndarray): data observations for which to sample

    Returns:
        np.ndarray: marginal sample
    &#34;&#34;&#34;

    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        X_obs_LE = ohe_to_le(X_obs, kwargs[&#34;agg_map&#34;])

        # Uniformly sample the marginals/features
        idx = np.random.randint(len(X_LE), size=X_obs_LE.shape)
        ret_mat = X_LE[idx, np.arange(X_obs_LE.shape[1])]
        return ret_mat

    idx = np.random.randint(len(X), size=X_obs.shape)
    return X[idx, np.arange(X_obs.shape[1])]


def gaussian_blur_permutation(
    X: np.ndarray, sigma: int, iterations=1000, **kwargs
) -&gt; np.ndarray:
    &#34;&#34;&#34;Gaussian blur over permuted features

    Args:
        X (np.ndarray): Source data for guassian blur
        sigma (int): Gaussian filter sigma
        iterations (int, optional): Number of permutations to average over.
                                    Defaults to 1000.

    Returns:
        np.ndarray: blurred data
    &#34;&#34;&#34;

    shuffled_gaussian_X = np.zeros_like(X).astype(float)
    d = X.shape[1]

    # Generate unique permutations of features
    permutations = []
    perms = set()
    for _ in range(iterations):
        while True:
            perm = permutation(d)
            key = tuple(perm)
            if key not in perms:
                perms.update(key)
                permutations.append(perm)
                break

    # Average gaussian blur over permutations
    for p in permutations:
        shuffled_gaussian_X += gaussian_filter(X[:, p], sigma)

    shuffled_gaussian_X /= iterations

    return shuffled_gaussian_X


def training(X: np.ndarray, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34;Training data distribution

    Args:
        X (np.ndarray): training data

    Returns:
        np.array: train dataset
    &#34;&#34;&#34;

    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        return X_LE

    return X


def opposite_class(
    X: np.ndarray,
    y: np.ndarray,
    pred_y_obs: np.ndarray,
    nsamples: int,
    **kwargs,
) -&gt; np.ndarray:
    &#34;&#34;&#34;Samples with an opposite label from the observation prediction

    Args:
        X (np.ndarray): training data
        y (np.ndarray): class of training data
        pred_y_obs (np.ndarray): predicted class of observations
        nsamples (int): Number of samples
    Returns:
        np.array: sample of training data with opposite class
    &#34;&#34;&#34;
    assert (
        nsamples is not None
    ), &#34;nsamples must be specified for opposite_class&#34;

    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])

        class_dict = {
            y_: sample(X_LE[y != y_], nsamples, random_state=None)
            for y_ in np.unique(y)
        }

        sample_sizes = [len(s) for s in class_dict.values()]
        assert all(s == min(sample_sizes) for s in sample_sizes), (
            f&#34;Opposite class can only support a maximum sample size of {min(sample_sizes)}. &#34;
            &#34;The sample size is constrained by the smallest class in the training data. &#34;
            &#34;Please either use more data or decrease nsamples for opposite_class.&#34;
        )

        return np.array([class_dict[y_] for y_ in pred_y_obs])

    class_dict = {
        y_: sample(X[y != y_], nsamples, random_state=None)
        for y_ in np.unique(y)
    }

    sample_sizes = [len(s) for s in class_dict.values()]
    assert all(s == min(sample_sizes) for s in sample_sizes), (
        f&#34;Opposite class can only support a maximum sample size of {min(sample_sizes)}. &#34;
        &#34;The sample size is constrained by the smallest class in the training data. &#34;
        &#34;Please either use more data or decrease nsamples for opposite_class.&#34;
    )

    return np.array([class_dict[y_] for y_ in pred_y_obs])


def nearest_neighbors(
    X: np.ndarray, X_obs: np.ndarray, k: int, **kwargs
) -&gt; np.ndarray:
    &#34;&#34;&#34;Nearest neighbors from reference set

    Args:
        X (np.ndarray): training data
        X_obs (np.ndarray): data observations for which to generate neighbors
        k (int): number of neighbors

    Returns:
        np.array: nearest neighbors
    &#34;&#34;&#34;

    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        X_obs_LE = ohe_to_le(X_obs, kwargs[&#34;agg_map&#34;])
        nbrs = NearestNeighbors(n_neighbors=k, algorithm=&#34;ball_tree&#34;).fit(X_LE)
        nn = nbrs.kneighbors(X_obs_LE, return_distance=False)
        return X_LE[nn]

    nbrs = NearestNeighbors(n_neighbors=k, algorithm=&#34;ball_tree&#34;).fit(X)
    nn = nbrs.kneighbors(X_obs, return_distance=False)
    return X[nn]


def nearest_neighbors_counterfactual(
    X: np.ndarray,
    y: np.ndarray,
    X_obs: np.ndarray,
    pred_y_obs: np.ndarray,
    k: int,
    **kwargs,
) -&gt; np.ndarray:
    &#34;&#34;&#34;Nearest neighbors from reference set having opposite class

    Args:
        X (np.ndarray): training data
        y (np.ndarray): class of training data
        X_obs (np.ndarray): data observations for which to generate neighbors
        pred_y_obs (np.ndarray): predicted class of observations
        k (int): number of neighbors

    Returns:
        np.array: nearest neighbors
    &#34;&#34;&#34;
    classes = np.unique(y)

    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        X_obs_LE = ohe_to_le(X_obs, kwargs[&#34;agg_map&#34;])

        # Create NN for each class containing other classes
        class_dict = {
            y_: NearestNeighbors(n_neighbors=k, algorithm=&#34;ball_tree&#34;).fit(
                X_LE[y != y_]
            )
            for y_ in classes
        }
        # Get indices of nn for each observation based on predicted class
        nn = np.concatenate(
            [
                class_dict[y_].kneighbors(
                    x.reshape(1, -1), return_distance=False
                )
                for (x, y_) in zip(X_obs_LE, pred_y_obs)
            ],
            axis=0,
        )

        return X_LE[nn]

    # Create NN for each class containing other classes
    class_dict = {
        y_: NearestNeighbors(n_neighbors=k, algorithm=&#34;ball_tree&#34;).fit(
            X[y != y_]
        )
        for y_ in classes
    }
    # Get indices of nn for each observation based on predicted class
    nn = np.concatenate(
        [
            class_dict[y_].kneighbors(x.reshape(1, -1), return_distance=False)
            for (x, y_) in zip(X_obs, pred_y_obs)
        ],
        axis=0,
    )

    return X[nn]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ablation.distributions.categorical_perturbation_case"><code class="name flex">
<span>def <span class="ident">categorical_perturbation_case</span></span>(<span>**kwargs) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Check for categoricals that need special care
during distribution generation.
The categoricals
should only be treated this way during perturbation.
Baselines should use the default format.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>Boolean indicating if distribution should be
handling categorical features another way.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def categorical_perturbation_case(**kwargs) -&gt; bool:
    &#34;&#34;&#34;
    Check for categoricals that need special care
    during distribution generation.  The categoricals
    should only be treated this way during perturbation.
    Baselines should use the default format.

    Returns:
        bool: Boolean indicating if distribution should be
              handling categorical features another way.
    &#34;&#34;&#34;
    if (
        (&#34;agg_map&#34; in kwargs)
        and (&#34;baseline&#34; not in kwargs)
        and (kwargs[&#34;agg_map&#34;] != None)
    ):
        return True
    return False</code></pre>
</details>
</dd>
<dt id="ablation.distributions.constant"><code class="name flex">
<span>def <span class="ident">constant</span></span>(<span>X: numpy.ndarray, value: Optional[float] = 0.0, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Generate a constant distribution</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Data to get shape</dd>
<dt><strong><code>value</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Constant value. Defaults to 0.0.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>constant distribution</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def constant(
    X: np.ndarray, value: Optional[float] = 0.0, **kwargs
) -&gt; np.ndarray:
    &#34;&#34;&#34;Generate a constant distribution

    Args:
        X (np.ndarray): Data to get shape
        value (float, optional): Constant value. Defaults to 0.0.

    Returns:
        np.array: constant distribution
    &#34;&#34;&#34;
    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        return np.ones((1, X_LE.shape[1])) * value

    return np.ones((1, X.shape[1])) * value</code></pre>
</details>
</dd>
<dt id="ablation.distributions.constant_mean"><code class="name flex">
<span>def <span class="ident">constant_mean</span></span>(<span>X: numpy.ndarray, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Generate a constant mean distribution (mean value per feature)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>data to derive mean</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>constant mean distribution</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def constant_mean(X: np.ndarray, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34;Generate a constant mean distribution (mean value per feature)

    Args:
        X (np.ndarray): data to derive mean

    Returns:
        np.array: constant mean distribution
    &#34;&#34;&#34;
    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        return np.mean(X_LE, axis=0, keepdims=True)

    return np.mean(X, axis=0, keepdims=True)</code></pre>
</details>
</dd>
<dt id="ablation.distributions.constant_median"><code class="name flex">
<span>def <span class="ident">constant_median</span></span>(<span>X: numpy.ndarray, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Generate a constant median distribution (median value per feature)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>data to derive median</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>constant median distribution</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def constant_median(X: np.ndarray, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34;Generate a constant median distribution (median value per feature)

    Args:
        X (np.ndarray): data to derive median

    Returns:
        np.array: constant median distribution
    &#34;&#34;&#34;
    if categorical_perturbation_case(**kwargs):
        # For this case, median is calculated over
        # numerical features and mode is calculated
        # over the categorical features.
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        median_mode = np.zeros((1, X_LE.shape[1]))  # To keep dimensions
        for (idx, mapping) in enumerate(kwargs[&#34;agg_map&#34;]):
            if len(mapping) &gt; 1:
                median_mode[0][idx] = stats.mode(X_LE[:, idx], axis=0)[0][
                    0
                ].astype(int)
            else:
                median_mode[0][idx] = np.median(X_LE[:, idx], axis=0)
        return median_mode

    return np.median(X, axis=0, keepdims=True)</code></pre>
</details>
</dd>
<dt id="ablation.distributions.gaussian"><code class="name flex">
<span>def <span class="ident">gaussian</span></span>(<span>X: numpy.ndarray, sigma: int, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Gaussian noise distribution</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>source data</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>int</code></dt>
<dd>sd of noise</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>noisy source data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gaussian(X: np.ndarray, sigma: int, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34;Gaussian noise distribution

    Args:
        X (np.ndarray): source data
        sigma (int): sd of noise

    Returns:
        np.array: noisy source data
    &#34;&#34;&#34;

    if categorical_perturbation_case(**kwargs):
        raise NotImplementedError(
            &#34;Gaussian perturbation has not been implemented for categorical feature types.&#34;
        )

    return np.clip(randn(*X.shape) * sigma + X, a_min=X.min(), a_max=X.max())</code></pre>
</details>
</dd>
<dt id="ablation.distributions.gaussian_blur"><code class="name flex">
<span>def <span class="ident">gaussian_blur</span></span>(<span>X: numpy.ndarray, sigma: int, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Generate gaussian blur over features</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>source data for guassian blur</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>int</code></dt>
<dd>Gaussian filter sigma</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>blurred data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gaussian_blur(X: np.ndarray, sigma: int, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34;Generate gaussian blur over features

    Args:
        X (np.ndarray): source data for guassian blur
        sigma (int): Gaussian filter sigma

    Returns:
        np.ndarray: blurred data
    &#34;&#34;&#34;

    if categorical_perturbation_case(**kwargs):
        raise NotImplementedError(
            &#34;Gaussian Blur perturbation has not been implemented for categorical feature types.&#34;
        )

    return gaussian_filter(X, sigma=sigma)</code></pre>
</details>
</dd>
<dt id="ablation.distributions.gaussian_blur_permutation"><code class="name flex">
<span>def <span class="ident">gaussian_blur_permutation</span></span>(<span>X: numpy.ndarray, sigma: int, iterations=1000, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Gaussian blur over permuted features</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Source data for guassian blur</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>int</code></dt>
<dd>Gaussian filter sigma</dd>
<dt><strong><code>iterations</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of permutations to average over.
Defaults to 1000.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>blurred data</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gaussian_blur_permutation(
    X: np.ndarray, sigma: int, iterations=1000, **kwargs
) -&gt; np.ndarray:
    &#34;&#34;&#34;Gaussian blur over permuted features

    Args:
        X (np.ndarray): Source data for guassian blur
        sigma (int): Gaussian filter sigma
        iterations (int, optional): Number of permutations to average over.
                                    Defaults to 1000.

    Returns:
        np.ndarray: blurred data
    &#34;&#34;&#34;

    shuffled_gaussian_X = np.zeros_like(X).astype(float)
    d = X.shape[1]

    # Generate unique permutations of features
    permutations = []
    perms = set()
    for _ in range(iterations):
        while True:
            perm = permutation(d)
            key = tuple(perm)
            if key not in perms:
                perms.update(key)
                permutations.append(perm)
                break

    # Average gaussian blur over permutations
    for p in permutations:
        shuffled_gaussian_X += gaussian_filter(X[:, p], sigma)

    shuffled_gaussian_X /= iterations

    return shuffled_gaussian_X</code></pre>
</details>
</dd>
<dt id="ablation.distributions.marginal"><code class="name flex">
<span>def <span class="ident">marginal</span></span>(<span>X: numpy.ndarray, X_obs: numpy.ndarray, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Sample over marginal distribution</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Source data for marginals</dd>
<dt><strong><code>X_obs</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>data observations for which to sample</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>marginal sample</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def marginal(X: np.ndarray, X_obs: np.ndarray, **kwargs) -&gt; np.ndarray:

    &#34;&#34;&#34;
    Sample over marginal distribution

    Args:
        X (np.ndarray): Source data for marginals
        X_obs (np.ndarray): data observations for which to sample

    Returns:
        np.ndarray: marginal sample
    &#34;&#34;&#34;

    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        X_obs_LE = ohe_to_le(X_obs, kwargs[&#34;agg_map&#34;])

        # Uniformly sample the marginals/features
        idx = np.random.randint(len(X_LE), size=X_obs_LE.shape)
        ret_mat = X_LE[idx, np.arange(X_obs_LE.shape[1])]
        return ret_mat

    idx = np.random.randint(len(X), size=X_obs.shape)
    return X[idx, np.arange(X_obs.shape[1])]</code></pre>
</details>
</dd>
<dt id="ablation.distributions.max_distance"><code class="name flex">
<span>def <span class="ident">max_distance</span></span>(<span>X: numpy.ndarray, X_obs: numpy.ndarray, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Furthest valid data sample in L1 distance</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>data to derive min/max</dd>
<dt><strong><code>X_obs</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>data observations for which to generate max_distance</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>furthest valid data samples by L1 distance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max_distance(X: np.ndarray, X_obs: np.ndarray, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34;Furthest valid data sample in L1 distance

    Args:
        X (np.ndarray): data to derive min/max
        X_obs (np.ndarray): data observations for which to generate max_distance

    Returns:
        np.array: furthest valid data samples by L1 distance
    &#34;&#34;&#34;

    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        X_obs_LE = ohe_to_le(X_obs, kwargs[&#34;agg_map&#34;])

        max_value = np.tile(X_LE.max(axis=0), (len(X_obs_LE), 1))
        min_value = np.tile(X_LE.min(axis=0), (len(X_obs_LE), 1))
        midpoint = (min_value + max_value) / 2

        # Maximum distance implemented for numericals.
        # Categoricals are uniformly sampled instead.
        modified_max_distance = np.zeros(
            (X_obs_LE.shape[0], X_obs_LE.shape[1])
        )

        for (idx, mapping) in enumerate(kwargs[&#34;agg_map&#34;]):
            if len(mapping) &gt; 1:
                # Replace categorical with uniformly random draw of the
                # other potential categories.

                unique_vals = np.unique(X_LE[:, idx])
                ps = np.apply_along_axis(
                    lambda x, y: np.setdiff1d(y, x),
                    1,
                    X_obs_LE[:, idx, np.newaxis],
                    unique_vals,
                )
                replacements = np.apply_along_axis(np.random.choice, 1, ps)
                modified_max_distance[:, idx] = replacements
            else:
                modified_max_distance[:, idx] = np.where(
                    X_obs_LE[:, idx] &lt; midpoint[:, idx],
                    max_value[:, idx],
                    min_value[:, idx],
                )
        return modified_max_distance

    max_value = np.tile(X.max(axis=0), (len(X_obs), 1))
    min_value = np.tile(X.min(axis=0), (len(X_obs), 1))
    midpoint = (min_value + max_value) / 2
    return np.where(X_obs &lt; midpoint, max_value, min_value)</code></pre>
</details>
</dd>
<dt id="ablation.distributions.nearest_neighbors"><code class="name flex">
<span>def <span class="ident">nearest_neighbors</span></span>(<span>X: numpy.ndarray, X_obs: numpy.ndarray, k: int, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Nearest neighbors from reference set</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>training data</dd>
<dt><strong><code>X_obs</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>data observations for which to generate neighbors</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>number of neighbors</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>nearest neighbors</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nearest_neighbors(
    X: np.ndarray, X_obs: np.ndarray, k: int, **kwargs
) -&gt; np.ndarray:
    &#34;&#34;&#34;Nearest neighbors from reference set

    Args:
        X (np.ndarray): training data
        X_obs (np.ndarray): data observations for which to generate neighbors
        k (int): number of neighbors

    Returns:
        np.array: nearest neighbors
    &#34;&#34;&#34;

    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        X_obs_LE = ohe_to_le(X_obs, kwargs[&#34;agg_map&#34;])
        nbrs = NearestNeighbors(n_neighbors=k, algorithm=&#34;ball_tree&#34;).fit(X_LE)
        nn = nbrs.kneighbors(X_obs_LE, return_distance=False)
        return X_LE[nn]

    nbrs = NearestNeighbors(n_neighbors=k, algorithm=&#34;ball_tree&#34;).fit(X)
    nn = nbrs.kneighbors(X_obs, return_distance=False)
    return X[nn]</code></pre>
</details>
</dd>
<dt id="ablation.distributions.nearest_neighbors_counterfactual"><code class="name flex">
<span>def <span class="ident">nearest_neighbors_counterfactual</span></span>(<span>X: numpy.ndarray, y: numpy.ndarray, X_obs: numpy.ndarray, pred_y_obs: numpy.ndarray, k: int, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Nearest neighbors from reference set having opposite class</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>training data</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>class of training data</dd>
<dt><strong><code>X_obs</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>data observations for which to generate neighbors</dd>
<dt><strong><code>pred_y_obs</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>predicted class of observations</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>number of neighbors</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>nearest neighbors</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nearest_neighbors_counterfactual(
    X: np.ndarray,
    y: np.ndarray,
    X_obs: np.ndarray,
    pred_y_obs: np.ndarray,
    k: int,
    **kwargs,
) -&gt; np.ndarray:
    &#34;&#34;&#34;Nearest neighbors from reference set having opposite class

    Args:
        X (np.ndarray): training data
        y (np.ndarray): class of training data
        X_obs (np.ndarray): data observations for which to generate neighbors
        pred_y_obs (np.ndarray): predicted class of observations
        k (int): number of neighbors

    Returns:
        np.array: nearest neighbors
    &#34;&#34;&#34;
    classes = np.unique(y)

    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        X_obs_LE = ohe_to_le(X_obs, kwargs[&#34;agg_map&#34;])

        # Create NN for each class containing other classes
        class_dict = {
            y_: NearestNeighbors(n_neighbors=k, algorithm=&#34;ball_tree&#34;).fit(
                X_LE[y != y_]
            )
            for y_ in classes
        }
        # Get indices of nn for each observation based on predicted class
        nn = np.concatenate(
            [
                class_dict[y_].kneighbors(
                    x.reshape(1, -1), return_distance=False
                )
                for (x, y_) in zip(X_obs_LE, pred_y_obs)
            ],
            axis=0,
        )

        return X_LE[nn]

    # Create NN for each class containing other classes
    class_dict = {
        y_: NearestNeighbors(n_neighbors=k, algorithm=&#34;ball_tree&#34;).fit(
            X[y != y_]
        )
        for y_ in classes
    }
    # Get indices of nn for each observation based on predicted class
    nn = np.concatenate(
        [
            class_dict[y_].kneighbors(x.reshape(1, -1), return_distance=False)
            for (x, y_) in zip(X_obs, pred_y_obs)
        ],
        axis=0,
    )

    return X[nn]</code></pre>
</details>
</dd>
<dt id="ablation.distributions.opposite_class"><code class="name flex">
<span>def <span class="ident">opposite_class</span></span>(<span>X: numpy.ndarray, y: numpy.ndarray, pred_y_obs: numpy.ndarray, nsamples: int, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Samples with an opposite label from the observation prediction</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>training data</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>class of training data</dd>
<dt><strong><code>pred_y_obs</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>predicted class of observations</dd>
<dt><strong><code>nsamples</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of samples</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>sample of training data with opposite class</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def opposite_class(
    X: np.ndarray,
    y: np.ndarray,
    pred_y_obs: np.ndarray,
    nsamples: int,
    **kwargs,
) -&gt; np.ndarray:
    &#34;&#34;&#34;Samples with an opposite label from the observation prediction

    Args:
        X (np.ndarray): training data
        y (np.ndarray): class of training data
        pred_y_obs (np.ndarray): predicted class of observations
        nsamples (int): Number of samples
    Returns:
        np.array: sample of training data with opposite class
    &#34;&#34;&#34;
    assert (
        nsamples is not None
    ), &#34;nsamples must be specified for opposite_class&#34;

    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])

        class_dict = {
            y_: sample(X_LE[y != y_], nsamples, random_state=None)
            for y_ in np.unique(y)
        }

        sample_sizes = [len(s) for s in class_dict.values()]
        assert all(s == min(sample_sizes) for s in sample_sizes), (
            f&#34;Opposite class can only support a maximum sample size of {min(sample_sizes)}. &#34;
            &#34;The sample size is constrained by the smallest class in the training data. &#34;
            &#34;Please either use more data or decrease nsamples for opposite_class.&#34;
        )

        return np.array([class_dict[y_] for y_ in pred_y_obs])

    class_dict = {
        y_: sample(X[y != y_], nsamples, random_state=None)
        for y_ in np.unique(y)
    }

    sample_sizes = [len(s) for s in class_dict.values()]
    assert all(s == min(sample_sizes) for s in sample_sizes), (
        f&#34;Opposite class can only support a maximum sample size of {min(sample_sizes)}. &#34;
        &#34;The sample size is constrained by the smallest class in the training data. &#34;
        &#34;Please either use more data or decrease nsamples for opposite_class.&#34;
    )

    return np.array([class_dict[y_] for y_ in pred_y_obs])</code></pre>
</details>
</dd>
<dt id="ablation.distributions.permutation"><code class="name flex">
<span>def <span class="ident">permutation</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>Randomly permute a sequence, or return a permuted range.</p>
<p>If <code>x</code> is a multi-dimensional array, it is only shuffled along its
first index.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code><a title="ablation.distributions.permutation" href="#ablation.distributions.permutation">RandomState.permutation()</a></code> method of a <code>default_rng()</code>
instance instead; please see the :ref:<code>random-quick-start</code>.</p>
</div>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>int</code> or <code>array_like</code></dt>
<dd>If <code>x</code> is an integer, randomly permute <code>np.arange(x)</code>.
If <code>x</code> is an array, make a copy and shuffle the elements
randomly.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Permuted sequence or array range.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>random.Generator.permutation</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.permutation(10)
array([1, 7, 4, 3, 0, 9, 2, 5, 8, 6]) # random
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.permutation([1, 4, 9, 12, 15])
array([15,  1,  9,  4, 12]) # random
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; arr = np.arange(9).reshape((3, 3))
&gt;&gt;&gt; np.random.permutation(arr)
array([[6, 7, 8], # random
       [0, 1, 2],
       [3, 4, 5]])
</code></pre></div>
</dd>
<dt id="ablation.distributions.randn"><code class="name flex">
<span>def <span class="ident">randn</span></span>(<span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>randn(d0, d1, &hellip;, dn)</p>
<p>Return a sample (or samples) from the "standard normal" distribution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a convenience function for users porting code from Matlab,
and wraps <code>standard_normal</code>. That function takes a
tuple to specify the size of the output, which is consistent with
other NumPy functions like <code>numpy.zeros</code> and <code>numpy.ones</code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>New code should use the <code>standard_normal</code> method of a <code>default_rng()</code>
instance instead; please see the :ref:<code>random-quick-start</code>.</p>
</div>
<p>If positive int_like arguments are provided, <code><a title="ablation.distributions.randn" href="#ablation.distributions.randn">RandomState.randn()</a></code> generates an array
of shape <code>(d0, d1, &hellip;, dn)</code>, filled
with random floats sampled from a univariate "normal" (Gaussian)
distribution of mean 0 and variance 1. A single float randomly sampled
from the distribution is returned if no argument is provided.</p>
<h2 id="parameters">Parameters</h2>
<p>d0, d1, &hellip;, dn : int, optional
The dimensions of the returned array, must be non-negative.
If no argument is given a single Python float is returned.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Z</code></strong> :&ensp;<code>ndarray</code> or <code>float</code></dt>
<dd>A <code>(d0, d1, &hellip;, dn)</code>-shaped array of floating-point samples from
the standard normal distribution, or a single such float if
no parameters were supplied.</dd>
</dl>
<h2 id="see-also">See Also</h2>
<dl>
<dt><code>standard_normal</code></dt>
<dd>Similar, but takes a tuple as its argument.</dd>
<dt><code>normal</code></dt>
<dd>Also accepts mu and sigma arguments.</dd>
<dt><code>random.Generator.standard_normal</code></dt>
<dd>which should be used for new code.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>For random samples from :math:<code>N(\mu, \sigma^2)</code>, use:</p>
<p><code>sigma * np.random.randn(...) + mu</code></p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; np.random.randn()
2.1923875335537315  # random
</code></pre>
<p>Two-by-four array of samples from N(3, 6.25):</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; 3 + 2.5 * np.random.randn(2, 4)
array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random
       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random
</code></pre></div>
</dd>
<dt id="ablation.distributions.training"><code class="name flex">
<span>def <span class="ident">training</span></span>(<span>X: numpy.ndarray, **kwargs) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Training data distribution</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>training data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>train dataset</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def training(X: np.ndarray, **kwargs) -&gt; np.ndarray:
    &#34;&#34;&#34;Training data distribution

    Args:
        X (np.ndarray): training data

    Returns:
        np.array: train dataset
    &#34;&#34;&#34;

    if categorical_perturbation_case(**kwargs):
        X_LE = ohe_to_le(X, kwargs[&#34;agg_map&#34;])
        return X_LE

    return X</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ablation" href="index.html">ablation</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ablation.distributions.categorical_perturbation_case" href="#ablation.distributions.categorical_perturbation_case">categorical_perturbation_case</a></code></li>
<li><code><a title="ablation.distributions.constant" href="#ablation.distributions.constant">constant</a></code></li>
<li><code><a title="ablation.distributions.constant_mean" href="#ablation.distributions.constant_mean">constant_mean</a></code></li>
<li><code><a title="ablation.distributions.constant_median" href="#ablation.distributions.constant_median">constant_median</a></code></li>
<li><code><a title="ablation.distributions.gaussian" href="#ablation.distributions.gaussian">gaussian</a></code></li>
<li><code><a title="ablation.distributions.gaussian_blur" href="#ablation.distributions.gaussian_blur">gaussian_blur</a></code></li>
<li><code><a title="ablation.distributions.gaussian_blur_permutation" href="#ablation.distributions.gaussian_blur_permutation">gaussian_blur_permutation</a></code></li>
<li><code><a title="ablation.distributions.marginal" href="#ablation.distributions.marginal">marginal</a></code></li>
<li><code><a title="ablation.distributions.max_distance" href="#ablation.distributions.max_distance">max_distance</a></code></li>
<li><code><a title="ablation.distributions.nearest_neighbors" href="#ablation.distributions.nearest_neighbors">nearest_neighbors</a></code></li>
<li><code><a title="ablation.distributions.nearest_neighbors_counterfactual" href="#ablation.distributions.nearest_neighbors_counterfactual">nearest_neighbors_counterfactual</a></code></li>
<li><code><a title="ablation.distributions.opposite_class" href="#ablation.distributions.opposite_class">opposite_class</a></code></li>
<li><code><a title="ablation.distributions.permutation" href="#ablation.distributions.permutation">permutation</a></code></li>
<li><code><a title="ablation.distributions.randn" href="#ablation.distributions.randn">randn</a></code></li>
<li><code><a title="ablation.distributions.training" href="#ablation.distributions.training">training</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>