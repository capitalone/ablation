<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ablation.experiment API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ablation.experiment</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import logging
import os
import pickle
import shutil
import time
from copy import copy
from typing import Any, Dict, List, Union
from warnings import warn

import numpy as np
import pandas as pd
import yaml

from .ablation import Ablation
from .baseline import generate_baseline_distribution
from .dataset import load_data
from .explanations import Explanations
from .perturb import generate_perturbation_distribution
from .pytorch_explanations import captum_explanation
from .pytorch_model import load_model, train
from .utils.evaluate import eval_model_performance
from .utils.logging import logger
from .utils.model import _predict_fn
from .utils.transform import le_to_ohe, ohe_to_le


class Config:
    def __init__(
        self,
        dataset_name: str,
        model_type: str,
        perturbation_config: Dict[str, Dict[str, int]],
        baseline_config: Dict[str, Dict[str, int]],
        explanation_methods: List[str],
        ablation_args: Dict[str, Any],
        dataset_sample_perc: float = 1.0,
        dataset_n_random_features: int = 0,
        n_trials: int = 1,
        path: Union[str, os.PathLike] = &#34;tmp&#34;,
        load=False,
        rerun_ablation=False,
    ):
        &#34;&#34;&#34;Experiment config

        Args:
            dataset_name (str): name of dataset
            model_type (str): model type (&#39;nn&#39; or &#39;linear&#39;)
            perturbation_config (Dict[str, Dict[str, int]]): dict of perturbation names associated with dicts of extra arguments
            baseline_config (Dict[str, Dict[str, int]]): dict of baseline names associated with dicts of extra arguments
            explanation_methods (List[str]): list of explanation methods
            ablation_args (Dict[str, Any]): dict of ablation arguments
            dataset_sample_perc (float): percent of dataset to use for experiment
            dataset_n_random_features (int): number of random features to add to dataset for sanity check
            n_trials (int): Number of seeds to run experiment
            path (str, optional): path to save results, model, and intermediary calculations. Defaults to &#34;tmp&#34;.
            load (bool, optional): If true, will load from path. Defaults to False.
            rerun_ablation (bool, optional): If true, will rerun ablation on load
        &#34;&#34;&#34;
        self.dataset_name = dataset_name
        self.model_type = model_type
        self.dataset_sample_perc = dataset_sample_perc
        self.dataset_n_random_features = dataset_n_random_features
        self.perturbation_config = perturbation_config
        self.baseline_config = baseline_config
        self.explanation_methods = explanation_methods
        self.ablation_args = ablation_args
        self.n_trials = n_trials
        self.path = path
        self.load = load
        self.rerun_ablation = rerun_ablation

        self._check()

    def _check(self):

        assert (
            isinstance(self.n_trials, int) and self.n_trials &gt;= 1
        ), &#34;n_trials must be an integer &gt;=1&#34;

        assert (
            len(self.perturbation_config) &gt; 0
        ), &#34;Must specify at least one perturbation&#34;

        assert (
            len(self.explanation_methods) &gt; 0
        ), &#34;Must specify at least one explanation method&#34;

        assert len(self.baseline_config) &gt; 0, &#34;Must specify at least one baseline&#34;

        assert self.model_type in [
            &#34;nn&#34;,
            &#34;linear&#34;,
        ], &#34;model_type must be type &#39;nn&#39; or &#39;linear&#39;&#34;

    @classmethod
    def from_yaml_file(cls, path: Union[str, os.PathLike]):
        config = yaml.safe_load(open(path, &#34;r&#34;))
        return cls(**config)

    @classmethod
    def from_dict(cls, dict_):
        return cls(**dict_)

    def to_dict(self):
        return copy(self.__dict__)

    def save(self):
        return yaml.dump(
            self.__dict__,
            open(os.path.join(self.path, &#34;config.yml&#34;), &#34;w&#34;),
            allow_unicode=True,
        )

    @classmethod
    def load(cls, path: Union[str, os.PathLike]):
        return cls.from_yaml_file(os.path.join(path, &#34;config.yml&#34;))

    @property
    def result_name(self):
        &#34;&#34;&#34;Name of results file based on ablation args&#34;&#34;&#34;
        explanation = &#34;local&#34; if self.ablation_args[&#34;local&#34;] else &#34;global&#34;
        return f&#34;results-{explanation}&#34;

    def diff(self, other: object) -&gt; bool:
        experiment_args = [
            &#34;dataset_name&#34;,
            &#34;perturbation_config&#34;,
            &#34;baseline_config&#34;,
            &#34;explanation_methods&#34;,
            &#34;ablation_args&#34;,
            &#34;dataset_sample_perc&#34;,
            &#34;dataset_n_random_features&#34;,
            &#34;n_trials&#34;,
            &#34;model_type&#34;,
        ]
        _self = self.__dict__
        _other = other.__dict__
        diff = [arg for arg in experiment_args if _self[arg] != _other[arg]]

        return diff

    def __eq__(self, other: object) -&gt; bool:
        return len(self.diff(other)) == 0


class Experiment:
    def __init__(self, config: Config):
        &#34;&#34;&#34;Experiment runner

        Args:
            config (Config): configuration
        &#34;&#34;&#34;
        self.config = config

        self._clean_dir()
        self._config_check()
        self.config.save()
        self._set_logging()

        self.dataset = load_data(
            self.config.dataset_name,
            self.config.dataset_sample_perc,
            self.config.dataset_n_random_features,
        )

        if self.config.load:
            self.model = load_model(self.config.path)
            self.label_shuffed_model = load_model(
                self.config.path, prefix=&#34;label_shuffed_model&#34;
            )
        else:
            self.model = train(
                self.dataset,
                max_epochs=100,
                path=self.config.path,
                model_type=self.config.model_type,
            )
            self.label_shuffed_model = train(
                self.dataset,
                max_epochs=100,
                path=self.config.path,
                model_type=self.config.model_type,
                prefix=&#34;label_shuffed_model&#34;,
                shuffle_labels=True,
            )

    def _set_logging(self):
        fh = logging.FileHandler(os.path.join(self.config.path, &#34;experiment.log&#34;))
        formatter = logging.Formatter(
            &#34;%(asctime)s - %(name)s - %(levelname)s - %(message)s&#34;
        )
        fh.setLevel(logging.INFO)
        fh.setFormatter(formatter)
        logger.addHandler(fh)

    def _config_check(self):
        self.rerun_ablation = self.config.rerun_ablation

        if self.config.load:
            if not os.path.exists(self.config.path):
                raise IOError(f&#34;Path does not exist: {self.config.path}&#34;)

            old_config = Config.load(self.config.path)
            diff = self.config.diff(old_config)

            if diff == [&#34;ablation_args&#34;]:
                warn(
                    &#34;Recomputing ablations from previous experiment with settings: &#34;
                    f&#34;{&#39;, &#39;.join([f&#39;{k}: {v}&#39; for k,v in self.config.ablation_args.items()])}.&#34;
                )
                self.rerun_ablation = True

            elif len(diff) &gt; 0:
                # TODO: Add more functionality to rerun components of the experiment
                raise ValueError(
                    f&#34;Configuration file doesn&#39;t match. The following arguments have changed: {&#39;, &#39;.join(diff)}.&#34;
                    # f&#34;Using config.yml from {self.config.path}.&#34;
                    # &#34;Set load = False to run new config.&#34;
                )

    def _clean_dir(self):
        &#34;&#34;&#34;Clean experiment directory&#34;&#34;&#34;
        if os.path.exists(self.config.path) and not self.config.load:
            shutil.rmtree(self.config.path, ignore_errors=True)

        if not os.path.exists(self.config.path):
            os.makedirs(self.config.path)

    def _load_if_exists(self, name):
        &#34;&#34;&#34;Load file if exists&#34;&#34;&#34;
        file = os.path.join(self.config.path, f&#34;{name}.pkl&#34;)
        if os.path.exists(file):
            return pickle.load(open(file, &#34;rb&#34;))
        return

    def _save(self, obj, name):
        &#34;&#34;&#34;Save object as pkl file&#34;&#34;&#34;
        file = os.path.join(self.config.path, f&#34;{name}.pkl&#34;)
        pickle.dump(obj, open(file, &#34;wb&#34;))

    def _compute_model_sanity_checks(self):
        self.model_sanity_checks = self._load_if_exists(&#34;model_sanity_checks&#34;)

        if self.model_sanity_checks is None:
            self.model_sanity_checks = {}

            self.model_sanity_checks[&#34;label_shuffled&#34;] = {
                scoring_method: eval_model_performance(
                    self.label_shuffed_model,
                    self.dataset.X_test,
                    self.dataset.y_test,
                    scoring_method=scoring_method,
                )
                for scoring_method in self.config.ablation_args[&#34;scoring_methods&#34;]
            }

            self._save(self.model_sanity_checks, &#34;model_sanity_checks&#34;)

        # TODO: Do we want also the default probabilities?
        # _, counts = np.unique(self.dataset.y_train, return_counts=True)
        # naive_pred = lambda x: np.tile(counts / sum(counts), (len(x), 1))
        # self.naive_performance = eval_model_performance(
        #     naive_pred,
        #     self.dataset.X_test,
        #     self.dataset.y_test,
        #     scoring_method=config.ablation_args[&#34;scoring_method&#34;],
        # )

    def _compute_perturbations(
        self, perturbation_config: Dict[str, Dict[str, int]]
    ) -&gt; None:
        &#34;&#34;&#34;Compute perturbations

        Args:
            perturbation_config (Dict[str, Dict[str, int]]): dict of perturbation names associated with dicts of extra arguments
        &#34;&#34;&#34;
        self.perturbations = self._load_if_exists(&#34;perturbations&#34;)
        if self.perturbations is None:
            self.perturbations = {
                trial: {
                    name: generate_perturbation_distribution(
                        name,
                        self.dataset.X_train,
                        self.dataset.X_test,
                        random_state=trial,
                        agg_map=self.dataset.agg_map,
                        **kwargs,
                    )
                    for name, kwargs in perturbation_config.items()
                }
                for trial in range(self.config.n_trials)
            }
            self._save(self.perturbations, &#34;perturbations&#34;)

    def _compute_baselines(self, baseline_config: Dict[str, Dict[str, int]]) -&gt; None:
        &#34;&#34;&#34;Compute baselines

        Args:
            baseline_config (Dict[str, Dict[str, int]]): dict of baseline names associated with dicts of extra arguments
        &#34;&#34;&#34;

        self.baselines = self._load_if_exists(&#34;baselines&#34;)

        if self.baselines is None:
            numpy_model = _predict_fn(self.model)
            y = numpy_model(self.dataset.X_train)
            y_obs = numpy_model(self.dataset.X_test)

            self.baselines = {
                trial: {
                    name: generate_baseline_distribution(
                        name,
                        self.dataset.X_train,
                        self.dataset.X_test,
                        y,
                        y_obs,
                        random_state=trial,
                        agg_map=self.dataset.agg_map,
                        **kwargs,
                    )
                    for name, kwargs in baseline_config.items()
                }
                for trial in range(self.config.n_trials)
            }
            self._save(self.baselines, &#34;baselines&#34;)

    def _compute_explanations(
        self,
        explanation_methods: List[str],
        computed_baselines: Dict[str, np.ndarray],
    ) -&gt; None:
        &#34;&#34;&#34;Compute explanations

        Args:
            explanation_methods (List[str]): list of explanations methods
            computed_baselines (Dict[str, np.ndarray]): dict of baseline name with associated baselines
        &#34;&#34;&#34;

        # TODO, we want to support raw explanations in addition to Explanations object
        self.explanations = self._load_if_exists(&#34;explanations&#34;)

        if self.explanations is None:
            self.explanations = {}
            non_random_exp_methods = [m for m in explanation_methods if m != &#34;random&#34;]
            for trial, baselines in computed_baselines.items():
                self.explanations[trial] = {}
                for method in non_random_exp_methods:
                    self.explanations[trial][method] = {}
                    for bname, baseline in baselines.items():
                        logger.info(
                            f&#34;Running explanation: {method} | baseline: {bname}&#34;
                        )

                        # Calculate the explanation values for set of observations
                        explanations = Explanations(
                            explanation_values=captum_explanation(
                                method,
                                self.model,
                                self.dataset.X_test,
                                baseline,
                                random_state=trial,
                            ),
                            agg_map=self.dataset.agg_map,
                        )

                        self.explanations[trial][method][bname] = explanations

                    if &#34;random&#34; in explanation_methods:
                        logger.info(&#34;Running random explanation&#34;)
                        explanations = Explanations(
                            explanation_values=captum_explanation(
                                &#34;random&#34;,
                                self.model,
                                self.dataset.X_test,
                                baseline,
                                random_state=trial,
                            ),
                            agg_map=self.dataset.agg_map,
                        )
                        self.explanations[trial][method][
                            &#34;random explanation&#34;
                        ] = explanations

            self._save(self.explanations, &#34;explanations&#34;)

    def run_exp(self):
        &#34;&#34;&#34;Run experiment&#34;&#34;&#34;

        self._compute_model_sanity_checks()
        self._compute_perturbations(self.config.perturbation_config)
        self._compute_baselines(self.config.baseline_config)
        self._compute_explanations(self.config.explanation_methods, self.baselines)

        self.results = self._load_if_exists(self.config.result_name)

        if self.results is None or self.rerun_ablation:

            trials = []
            for trial in range(self.config.n_trials):
                logger.info(f&#34;Running ablation trial {trial}&#34;)
                np.random.seed(trial)
                comb = []
                for p_name, perturb in self.perturbations[trial].items():
                    for exp_name, exp_dict in self.explanations[trial].items():
                        for b_name, exp in exp_dict.items():

                            # TODO: debug kernelshap and lime for overflow
                            # Below is currently a quick fix to exclude these samples
                            overflow_idx = np.unique(
                                np.where(exp.data(&#34;sparse&#34;) &gt; 10)[0]
                            )
                            if len(overflow_idx) &gt; 0:
                                logger.warn(
                                    f&#34;Overflow warning (perturb: {p_name}, exp: {exp_name}, baseline: {b_name}):\n&#34;
                                    f&#34;{len(overflow_idx)} samples removed. &#34;
                                    f&#34;Indices: {&#39;,&#39;.join(overflow_idx.astype(str))}&#34;
                                )

                            abl = Ablation(
                                perturbation_distribution=np.delete(
                                    perturb, overflow_idx, 0
                                ),
                                model=self.model,
                                dataset=self.dataset,
                                X=np.delete(self.dataset.X_test, overflow_idx, 0),
                                y=np.delete(self.dataset.y_test, overflow_idx, 0),
                                explanation_values=np.delete(
                                    exp.data(&#34;sparse&#34;), overflow_idx, 0
                                ),
                                explanation_values_dense=np.delete(
                                    exp.data(&#34;dense&#34;), overflow_idx, 0
                                ),
                                random_feat_idx=self.dataset.dense_random_feat_idx,
                                **self.config.ablation_args,
                            )
                            # abl = Ablation(
                            #     perturbation_distribution=perturb,
                            #     model=self.model,
                            #     X=self.dataset.X_test,
                            #     y=self.dataset.y_test,
                            #     explanation_values=exp,
                            #     random_feat_idx=self.dataset.random_feat_idx,
                            #     **self.config.ablation_args,
                            # )

                            result = abl.ablate_features()

                            n_obs = len(result[&#34;scores&#34;])
                            result[&#34;explanation_method&#34;] = [exp_name] * n_obs
                            result[&#34;baseline&#34;] = [b_name] * n_obs
                            result[&#34;perturbation&#34;] = [p_name] * n_obs
                            (
                                result[&#34;random_sanity_check_idx&#34;],
                                result[&#34;random_sanity_check_perc&#34;],
                            ) = abl.random_sanity_check_idx()
                            result[
                                &#34;random_sanity_check_value&#34;
                            ] = abl.random_sanity_check_value()
                            comb.append(result)

                trial_df = pd.concat(comb)
                trial_df[&#34;trial&#34;] = trial
                trials.append(trial_df)

            self.results = pd.concat([t for t in trials]).reset_index(drop=True)
            self._save(self.results, self.config.result_name)

        return self.results</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ablation.experiment.Config"><code class="flex name class">
<span>class <span class="ident">Config</span></span>
<span>(</span><span>dataset_name: str, model_type: str, perturbation_config: Dict[str, Dict[str, int]], baseline_config: Dict[str, Dict[str, int]], explanation_methods: List[str], ablation_args: Dict[str, Any], dataset_sample_perc: float = 1.0, dataset_n_random_features: int = 0, n_trials: int = 1, path: Union[str, os.PathLike] = 'tmp', load=False, rerun_ablation=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Experiment config</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset_name</code></strong> :&ensp;<code>str</code></dt>
<dd>name of dataset</dd>
<dt><strong><code>model_type</code></strong> :&ensp;<code>str</code></dt>
<dd>model type ('nn' or 'linear')</dd>
<dt><strong><code>perturbation_config</code></strong> :&ensp;<code>Dict[str, Dict[str, int]]</code></dt>
<dd>dict of perturbation names associated with dicts of extra arguments</dd>
<dt><strong><code>baseline_config</code></strong> :&ensp;<code>Dict[str, Dict[str, int]]</code></dt>
<dd>dict of baseline names associated with dicts of extra arguments</dd>
<dt><strong><code>explanation_methods</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>list of explanation methods</dd>
<dt><strong><code>ablation_args</code></strong> :&ensp;<code>Dict[str, Any]</code></dt>
<dd>dict of ablation arguments</dd>
<dt><strong><code>dataset_sample_perc</code></strong> :&ensp;<code>float</code></dt>
<dd>percent of dataset to use for experiment</dd>
<dt><strong><code>dataset_n_random_features</code></strong> :&ensp;<code>int</code></dt>
<dd>number of random features to add to dataset for sanity check</dd>
<dt><strong><code>n_trials</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of seeds to run experiment</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>path to save results, model, and intermediary calculations. Defaults to "tmp".</dd>
<dt><strong><code>load</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If true, will load from path. Defaults to False.</dd>
<dt><strong><code>rerun_ablation</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If true, will rerun ablation on load</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Config:
    def __init__(
        self,
        dataset_name: str,
        model_type: str,
        perturbation_config: Dict[str, Dict[str, int]],
        baseline_config: Dict[str, Dict[str, int]],
        explanation_methods: List[str],
        ablation_args: Dict[str, Any],
        dataset_sample_perc: float = 1.0,
        dataset_n_random_features: int = 0,
        n_trials: int = 1,
        path: Union[str, os.PathLike] = &#34;tmp&#34;,
        load=False,
        rerun_ablation=False,
    ):
        &#34;&#34;&#34;Experiment config

        Args:
            dataset_name (str): name of dataset
            model_type (str): model type (&#39;nn&#39; or &#39;linear&#39;)
            perturbation_config (Dict[str, Dict[str, int]]): dict of perturbation names associated with dicts of extra arguments
            baseline_config (Dict[str, Dict[str, int]]): dict of baseline names associated with dicts of extra arguments
            explanation_methods (List[str]): list of explanation methods
            ablation_args (Dict[str, Any]): dict of ablation arguments
            dataset_sample_perc (float): percent of dataset to use for experiment
            dataset_n_random_features (int): number of random features to add to dataset for sanity check
            n_trials (int): Number of seeds to run experiment
            path (str, optional): path to save results, model, and intermediary calculations. Defaults to &#34;tmp&#34;.
            load (bool, optional): If true, will load from path. Defaults to False.
            rerun_ablation (bool, optional): If true, will rerun ablation on load
        &#34;&#34;&#34;
        self.dataset_name = dataset_name
        self.model_type = model_type
        self.dataset_sample_perc = dataset_sample_perc
        self.dataset_n_random_features = dataset_n_random_features
        self.perturbation_config = perturbation_config
        self.baseline_config = baseline_config
        self.explanation_methods = explanation_methods
        self.ablation_args = ablation_args
        self.n_trials = n_trials
        self.path = path
        self.load = load
        self.rerun_ablation = rerun_ablation

        self._check()

    def _check(self):

        assert (
            isinstance(self.n_trials, int) and self.n_trials &gt;= 1
        ), &#34;n_trials must be an integer &gt;=1&#34;

        assert (
            len(self.perturbation_config) &gt; 0
        ), &#34;Must specify at least one perturbation&#34;

        assert (
            len(self.explanation_methods) &gt; 0
        ), &#34;Must specify at least one explanation method&#34;

        assert len(self.baseline_config) &gt; 0, &#34;Must specify at least one baseline&#34;

        assert self.model_type in [
            &#34;nn&#34;,
            &#34;linear&#34;,
        ], &#34;model_type must be type &#39;nn&#39; or &#39;linear&#39;&#34;

    @classmethod
    def from_yaml_file(cls, path: Union[str, os.PathLike]):
        config = yaml.safe_load(open(path, &#34;r&#34;))
        return cls(**config)

    @classmethod
    def from_dict(cls, dict_):
        return cls(**dict_)

    def to_dict(self):
        return copy(self.__dict__)

    def save(self):
        return yaml.dump(
            self.__dict__,
            open(os.path.join(self.path, &#34;config.yml&#34;), &#34;w&#34;),
            allow_unicode=True,
        )

    @classmethod
    def load(cls, path: Union[str, os.PathLike]):
        return cls.from_yaml_file(os.path.join(path, &#34;config.yml&#34;))

    @property
    def result_name(self):
        &#34;&#34;&#34;Name of results file based on ablation args&#34;&#34;&#34;
        explanation = &#34;local&#34; if self.ablation_args[&#34;local&#34;] else &#34;global&#34;
        return f&#34;results-{explanation}&#34;

    def diff(self, other: object) -&gt; bool:
        experiment_args = [
            &#34;dataset_name&#34;,
            &#34;perturbation_config&#34;,
            &#34;baseline_config&#34;,
            &#34;explanation_methods&#34;,
            &#34;ablation_args&#34;,
            &#34;dataset_sample_perc&#34;,
            &#34;dataset_n_random_features&#34;,
            &#34;n_trials&#34;,
            &#34;model_type&#34;,
        ]
        _self = self.__dict__
        _other = other.__dict__
        diff = [arg for arg in experiment_args if _self[arg] != _other[arg]]

        return diff

    def __eq__(self, other: object) -&gt; bool:
        return len(self.diff(other)) == 0</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="ablation.experiment.Config.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>dict_)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_dict(cls, dict_):
    return cls(**dict_)</code></pre>
</details>
</dd>
<dt id="ablation.experiment.Config.from_yaml_file"><code class="name flex">
<span>def <span class="ident">from_yaml_file</span></span>(<span>path: Union[str, os.PathLike])</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_yaml_file(cls, path: Union[str, os.PathLike]):
    config = yaml.safe_load(open(path, &#34;r&#34;))
    return cls(**config)</code></pre>
</details>
</dd>
<dt id="ablation.experiment.Config.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>path: Union[str, os.PathLike])</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load(cls, path: Union[str, os.PathLike]):
    return cls.from_yaml_file(os.path.join(path, &#34;config.yml&#34;))</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="ablation.experiment.Config.result_name"><code class="name">var <span class="ident">result_name</span></code></dt>
<dd>
<div class="desc"><p>Name of results file based on ablation args</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def result_name(self):
    &#34;&#34;&#34;Name of results file based on ablation args&#34;&#34;&#34;
    explanation = &#34;local&#34; if self.ablation_args[&#34;local&#34;] else &#34;global&#34;
    return f&#34;results-{explanation}&#34;</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ablation.experiment.Config.diff"><code class="name flex">
<span>def <span class="ident">diff</span></span>(<span>self, other: object) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def diff(self, other: object) -&gt; bool:
    experiment_args = [
        &#34;dataset_name&#34;,
        &#34;perturbation_config&#34;,
        &#34;baseline_config&#34;,
        &#34;explanation_methods&#34;,
        &#34;ablation_args&#34;,
        &#34;dataset_sample_perc&#34;,
        &#34;dataset_n_random_features&#34;,
        &#34;n_trials&#34;,
        &#34;model_type&#34;,
    ]
    _self = self.__dict__
    _other = other.__dict__
    diff = [arg for arg in experiment_args if _self[arg] != _other[arg]]

    return diff</code></pre>
</details>
</dd>
<dt id="ablation.experiment.Config.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self):
    return yaml.dump(
        self.__dict__,
        open(os.path.join(self.path, &#34;config.yml&#34;), &#34;w&#34;),
        allow_unicode=True,
    )</code></pre>
</details>
</dd>
<dt id="ablation.experiment.Config.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self):
    return copy(self.__dict__)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ablation.experiment.Experiment"><code class="flex name class">
<span>class <span class="ident">Experiment</span></span>
<span>(</span><span>config: <a title="ablation.experiment.Config" href="#ablation.experiment.Config">Config</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Experiment runner</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config</code></strong> :&ensp;<code><a title="ablation.experiment.Config" href="#ablation.experiment.Config">Config</a></code></dt>
<dd>configuration</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Experiment:
    def __init__(self, config: Config):
        &#34;&#34;&#34;Experiment runner

        Args:
            config (Config): configuration
        &#34;&#34;&#34;
        self.config = config

        self._clean_dir()
        self._config_check()
        self.config.save()
        self._set_logging()

        self.dataset = load_data(
            self.config.dataset_name,
            self.config.dataset_sample_perc,
            self.config.dataset_n_random_features,
        )

        if self.config.load:
            self.model = load_model(self.config.path)
            self.label_shuffed_model = load_model(
                self.config.path, prefix=&#34;label_shuffed_model&#34;
            )
        else:
            self.model = train(
                self.dataset,
                max_epochs=100,
                path=self.config.path,
                model_type=self.config.model_type,
            )
            self.label_shuffed_model = train(
                self.dataset,
                max_epochs=100,
                path=self.config.path,
                model_type=self.config.model_type,
                prefix=&#34;label_shuffed_model&#34;,
                shuffle_labels=True,
            )

    def _set_logging(self):
        fh = logging.FileHandler(os.path.join(self.config.path, &#34;experiment.log&#34;))
        formatter = logging.Formatter(
            &#34;%(asctime)s - %(name)s - %(levelname)s - %(message)s&#34;
        )
        fh.setLevel(logging.INFO)
        fh.setFormatter(formatter)
        logger.addHandler(fh)

    def _config_check(self):
        self.rerun_ablation = self.config.rerun_ablation

        if self.config.load:
            if not os.path.exists(self.config.path):
                raise IOError(f&#34;Path does not exist: {self.config.path}&#34;)

            old_config = Config.load(self.config.path)
            diff = self.config.diff(old_config)

            if diff == [&#34;ablation_args&#34;]:
                warn(
                    &#34;Recomputing ablations from previous experiment with settings: &#34;
                    f&#34;{&#39;, &#39;.join([f&#39;{k}: {v}&#39; for k,v in self.config.ablation_args.items()])}.&#34;
                )
                self.rerun_ablation = True

            elif len(diff) &gt; 0:
                # TODO: Add more functionality to rerun components of the experiment
                raise ValueError(
                    f&#34;Configuration file doesn&#39;t match. The following arguments have changed: {&#39;, &#39;.join(diff)}.&#34;
                    # f&#34;Using config.yml from {self.config.path}.&#34;
                    # &#34;Set load = False to run new config.&#34;
                )

    def _clean_dir(self):
        &#34;&#34;&#34;Clean experiment directory&#34;&#34;&#34;
        if os.path.exists(self.config.path) and not self.config.load:
            shutil.rmtree(self.config.path, ignore_errors=True)

        if not os.path.exists(self.config.path):
            os.makedirs(self.config.path)

    def _load_if_exists(self, name):
        &#34;&#34;&#34;Load file if exists&#34;&#34;&#34;
        file = os.path.join(self.config.path, f&#34;{name}.pkl&#34;)
        if os.path.exists(file):
            return pickle.load(open(file, &#34;rb&#34;))
        return

    def _save(self, obj, name):
        &#34;&#34;&#34;Save object as pkl file&#34;&#34;&#34;
        file = os.path.join(self.config.path, f&#34;{name}.pkl&#34;)
        pickle.dump(obj, open(file, &#34;wb&#34;))

    def _compute_model_sanity_checks(self):
        self.model_sanity_checks = self._load_if_exists(&#34;model_sanity_checks&#34;)

        if self.model_sanity_checks is None:
            self.model_sanity_checks = {}

            self.model_sanity_checks[&#34;label_shuffled&#34;] = {
                scoring_method: eval_model_performance(
                    self.label_shuffed_model,
                    self.dataset.X_test,
                    self.dataset.y_test,
                    scoring_method=scoring_method,
                )
                for scoring_method in self.config.ablation_args[&#34;scoring_methods&#34;]
            }

            self._save(self.model_sanity_checks, &#34;model_sanity_checks&#34;)

        # TODO: Do we want also the default probabilities?
        # _, counts = np.unique(self.dataset.y_train, return_counts=True)
        # naive_pred = lambda x: np.tile(counts / sum(counts), (len(x), 1))
        # self.naive_performance = eval_model_performance(
        #     naive_pred,
        #     self.dataset.X_test,
        #     self.dataset.y_test,
        #     scoring_method=config.ablation_args[&#34;scoring_method&#34;],
        # )

    def _compute_perturbations(
        self, perturbation_config: Dict[str, Dict[str, int]]
    ) -&gt; None:
        &#34;&#34;&#34;Compute perturbations

        Args:
            perturbation_config (Dict[str, Dict[str, int]]): dict of perturbation names associated with dicts of extra arguments
        &#34;&#34;&#34;
        self.perturbations = self._load_if_exists(&#34;perturbations&#34;)
        if self.perturbations is None:
            self.perturbations = {
                trial: {
                    name: generate_perturbation_distribution(
                        name,
                        self.dataset.X_train,
                        self.dataset.X_test,
                        random_state=trial,
                        agg_map=self.dataset.agg_map,
                        **kwargs,
                    )
                    for name, kwargs in perturbation_config.items()
                }
                for trial in range(self.config.n_trials)
            }
            self._save(self.perturbations, &#34;perturbations&#34;)

    def _compute_baselines(self, baseline_config: Dict[str, Dict[str, int]]) -&gt; None:
        &#34;&#34;&#34;Compute baselines

        Args:
            baseline_config (Dict[str, Dict[str, int]]): dict of baseline names associated with dicts of extra arguments
        &#34;&#34;&#34;

        self.baselines = self._load_if_exists(&#34;baselines&#34;)

        if self.baselines is None:
            numpy_model = _predict_fn(self.model)
            y = numpy_model(self.dataset.X_train)
            y_obs = numpy_model(self.dataset.X_test)

            self.baselines = {
                trial: {
                    name: generate_baseline_distribution(
                        name,
                        self.dataset.X_train,
                        self.dataset.X_test,
                        y,
                        y_obs,
                        random_state=trial,
                        agg_map=self.dataset.agg_map,
                        **kwargs,
                    )
                    for name, kwargs in baseline_config.items()
                }
                for trial in range(self.config.n_trials)
            }
            self._save(self.baselines, &#34;baselines&#34;)

    def _compute_explanations(
        self,
        explanation_methods: List[str],
        computed_baselines: Dict[str, np.ndarray],
    ) -&gt; None:
        &#34;&#34;&#34;Compute explanations

        Args:
            explanation_methods (List[str]): list of explanations methods
            computed_baselines (Dict[str, np.ndarray]): dict of baseline name with associated baselines
        &#34;&#34;&#34;

        # TODO, we want to support raw explanations in addition to Explanations object
        self.explanations = self._load_if_exists(&#34;explanations&#34;)

        if self.explanations is None:
            self.explanations = {}
            non_random_exp_methods = [m for m in explanation_methods if m != &#34;random&#34;]
            for trial, baselines in computed_baselines.items():
                self.explanations[trial] = {}
                for method in non_random_exp_methods:
                    self.explanations[trial][method] = {}
                    for bname, baseline in baselines.items():
                        logger.info(
                            f&#34;Running explanation: {method} | baseline: {bname}&#34;
                        )

                        # Calculate the explanation values for set of observations
                        explanations = Explanations(
                            explanation_values=captum_explanation(
                                method,
                                self.model,
                                self.dataset.X_test,
                                baseline,
                                random_state=trial,
                            ),
                            agg_map=self.dataset.agg_map,
                        )

                        self.explanations[trial][method][bname] = explanations

                    if &#34;random&#34; in explanation_methods:
                        logger.info(&#34;Running random explanation&#34;)
                        explanations = Explanations(
                            explanation_values=captum_explanation(
                                &#34;random&#34;,
                                self.model,
                                self.dataset.X_test,
                                baseline,
                                random_state=trial,
                            ),
                            agg_map=self.dataset.agg_map,
                        )
                        self.explanations[trial][method][
                            &#34;random explanation&#34;
                        ] = explanations

            self._save(self.explanations, &#34;explanations&#34;)

    def run_exp(self):
        &#34;&#34;&#34;Run experiment&#34;&#34;&#34;

        self._compute_model_sanity_checks()
        self._compute_perturbations(self.config.perturbation_config)
        self._compute_baselines(self.config.baseline_config)
        self._compute_explanations(self.config.explanation_methods, self.baselines)

        self.results = self._load_if_exists(self.config.result_name)

        if self.results is None or self.rerun_ablation:

            trials = []
            for trial in range(self.config.n_trials):
                logger.info(f&#34;Running ablation trial {trial}&#34;)
                np.random.seed(trial)
                comb = []
                for p_name, perturb in self.perturbations[trial].items():
                    for exp_name, exp_dict in self.explanations[trial].items():
                        for b_name, exp in exp_dict.items():

                            # TODO: debug kernelshap and lime for overflow
                            # Below is currently a quick fix to exclude these samples
                            overflow_idx = np.unique(
                                np.where(exp.data(&#34;sparse&#34;) &gt; 10)[0]
                            )
                            if len(overflow_idx) &gt; 0:
                                logger.warn(
                                    f&#34;Overflow warning (perturb: {p_name}, exp: {exp_name}, baseline: {b_name}):\n&#34;
                                    f&#34;{len(overflow_idx)} samples removed. &#34;
                                    f&#34;Indices: {&#39;,&#39;.join(overflow_idx.astype(str))}&#34;
                                )

                            abl = Ablation(
                                perturbation_distribution=np.delete(
                                    perturb, overflow_idx, 0
                                ),
                                model=self.model,
                                dataset=self.dataset,
                                X=np.delete(self.dataset.X_test, overflow_idx, 0),
                                y=np.delete(self.dataset.y_test, overflow_idx, 0),
                                explanation_values=np.delete(
                                    exp.data(&#34;sparse&#34;), overflow_idx, 0
                                ),
                                explanation_values_dense=np.delete(
                                    exp.data(&#34;dense&#34;), overflow_idx, 0
                                ),
                                random_feat_idx=self.dataset.dense_random_feat_idx,
                                **self.config.ablation_args,
                            )
                            # abl = Ablation(
                            #     perturbation_distribution=perturb,
                            #     model=self.model,
                            #     X=self.dataset.X_test,
                            #     y=self.dataset.y_test,
                            #     explanation_values=exp,
                            #     random_feat_idx=self.dataset.random_feat_idx,
                            #     **self.config.ablation_args,
                            # )

                            result = abl.ablate_features()

                            n_obs = len(result[&#34;scores&#34;])
                            result[&#34;explanation_method&#34;] = [exp_name] * n_obs
                            result[&#34;baseline&#34;] = [b_name] * n_obs
                            result[&#34;perturbation&#34;] = [p_name] * n_obs
                            (
                                result[&#34;random_sanity_check_idx&#34;],
                                result[&#34;random_sanity_check_perc&#34;],
                            ) = abl.random_sanity_check_idx()
                            result[
                                &#34;random_sanity_check_value&#34;
                            ] = abl.random_sanity_check_value()
                            comb.append(result)

                trial_df = pd.concat(comb)
                trial_df[&#34;trial&#34;] = trial
                trials.append(trial_df)

            self.results = pd.concat([t for t in trials]).reset_index(drop=True)
            self._save(self.results, self.config.result_name)

        return self.results</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="ablation.experiment.Experiment.run_exp"><code class="name flex">
<span>def <span class="ident">run_exp</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Run experiment</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_exp(self):
    &#34;&#34;&#34;Run experiment&#34;&#34;&#34;

    self._compute_model_sanity_checks()
    self._compute_perturbations(self.config.perturbation_config)
    self._compute_baselines(self.config.baseline_config)
    self._compute_explanations(self.config.explanation_methods, self.baselines)

    self.results = self._load_if_exists(self.config.result_name)

    if self.results is None or self.rerun_ablation:

        trials = []
        for trial in range(self.config.n_trials):
            logger.info(f&#34;Running ablation trial {trial}&#34;)
            np.random.seed(trial)
            comb = []
            for p_name, perturb in self.perturbations[trial].items():
                for exp_name, exp_dict in self.explanations[trial].items():
                    for b_name, exp in exp_dict.items():

                        # TODO: debug kernelshap and lime for overflow
                        # Below is currently a quick fix to exclude these samples
                        overflow_idx = np.unique(
                            np.where(exp.data(&#34;sparse&#34;) &gt; 10)[0]
                        )
                        if len(overflow_idx) &gt; 0:
                            logger.warn(
                                f&#34;Overflow warning (perturb: {p_name}, exp: {exp_name}, baseline: {b_name}):\n&#34;
                                f&#34;{len(overflow_idx)} samples removed. &#34;
                                f&#34;Indices: {&#39;,&#39;.join(overflow_idx.astype(str))}&#34;
                            )

                        abl = Ablation(
                            perturbation_distribution=np.delete(
                                perturb, overflow_idx, 0
                            ),
                            model=self.model,
                            dataset=self.dataset,
                            X=np.delete(self.dataset.X_test, overflow_idx, 0),
                            y=np.delete(self.dataset.y_test, overflow_idx, 0),
                            explanation_values=np.delete(
                                exp.data(&#34;sparse&#34;), overflow_idx, 0
                            ),
                            explanation_values_dense=np.delete(
                                exp.data(&#34;dense&#34;), overflow_idx, 0
                            ),
                            random_feat_idx=self.dataset.dense_random_feat_idx,
                            **self.config.ablation_args,
                        )
                        # abl = Ablation(
                        #     perturbation_distribution=perturb,
                        #     model=self.model,
                        #     X=self.dataset.X_test,
                        #     y=self.dataset.y_test,
                        #     explanation_values=exp,
                        #     random_feat_idx=self.dataset.random_feat_idx,
                        #     **self.config.ablation_args,
                        # )

                        result = abl.ablate_features()

                        n_obs = len(result[&#34;scores&#34;])
                        result[&#34;explanation_method&#34;] = [exp_name] * n_obs
                        result[&#34;baseline&#34;] = [b_name] * n_obs
                        result[&#34;perturbation&#34;] = [p_name] * n_obs
                        (
                            result[&#34;random_sanity_check_idx&#34;],
                            result[&#34;random_sanity_check_perc&#34;],
                        ) = abl.random_sanity_check_idx()
                        result[
                            &#34;random_sanity_check_value&#34;
                        ] = abl.random_sanity_check_value()
                        comb.append(result)

            trial_df = pd.concat(comb)
            trial_df[&#34;trial&#34;] = trial
            trials.append(trial_df)

        self.results = pd.concat([t for t in trials]).reset_index(drop=True)
        self._save(self.results, self.config.result_name)

    return self.results</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ablation" href="index.html">ablation</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ablation.experiment.Config" href="#ablation.experiment.Config">Config</a></code></h4>
<ul class="two-column">
<li><code><a title="ablation.experiment.Config.diff" href="#ablation.experiment.Config.diff">diff</a></code></li>
<li><code><a title="ablation.experiment.Config.from_dict" href="#ablation.experiment.Config.from_dict">from_dict</a></code></li>
<li><code><a title="ablation.experiment.Config.from_yaml_file" href="#ablation.experiment.Config.from_yaml_file">from_yaml_file</a></code></li>
<li><code><a title="ablation.experiment.Config.load" href="#ablation.experiment.Config.load">load</a></code></li>
<li><code><a title="ablation.experiment.Config.result_name" href="#ablation.experiment.Config.result_name">result_name</a></code></li>
<li><code><a title="ablation.experiment.Config.save" href="#ablation.experiment.Config.save">save</a></code></li>
<li><code><a title="ablation.experiment.Config.to_dict" href="#ablation.experiment.Config.to_dict">to_dict</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ablation.experiment.Experiment" href="#ablation.experiment.Experiment">Experiment</a></code></h4>
<ul class="">
<li><code><a title="ablation.experiment.Experiment.run_exp" href="#ablation.experiment.Experiment.run_exp">run_exp</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>